# -*- coding: utf-8 -*-
"""Keras_based_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a68sNPRfyUB6rEuywTTw2HUwPtCyQD--

<a id="p1"></a>

- Used Keras to fit a predictive model, classifying news articles into topics. 

"""

# Import data (don't alter the code in this cell)
from tensorflow.keras.datasets import reuters

# Suppress some warnings from deprecated reuters.load_data
import warnings
warnings.filterwarnings('ignore')

# Load data
(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=None,
                                                         skip_top=0,
                                                         maxlen=None,
                                                         test_split=0.2,
                                                         seed=723812,
                                                         start_char=1,
                                                         oov_char=2,
                                                         index_from=3)


train_size = 1000
X_train = X_train[:train_size]
y_train = y_train[:train_size]

# Demo of encoding
word_index = reuters.get_word_index(path="reuters_word_index.json")

print(f"Iran is encoded as {word_index['iran']} in the data")
print(f"London is encoded as {word_index['london']} in the data")
print("Words are encoded as numbers in our dataset.")

# Imports (don't alter this code)
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM

# DO NOT CHANGE THESE VALUES 
# Keras docs say that the + 1 is needed: https://keras.io/api/layers/core_layers/embedding/
MAX_FEATURES = len(word_index.values()) + 1

# maxlen is the length of each sequence (i.e. document length)
MAXLEN = 200

# Pre-process your data by creating sequences 


# YOUR CODE HERE
X_train = sequence.pad_sequences(X_train, maxlen=MAXLEN, padding='pre', truncating='post')
X_test = sequence.pad_sequences(X_test, maxlen=MAXLEN, padding='pre', truncating='post')


# Build and complie model


model = Sequential()

model.add(Embedding(input_dim=MAX_FEATURES, output_dim=128))

model.add(LSTM(128, return_sequences=False))

model.add(Dense(46, activation='softmax'))

model.compile(loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Visible Test
assert model.get_config()["layers"][1]["class_name"] == "Embedding", "Layer 1 should be an Embedding layer."

# Hidden Test

"""### Fit your model

Now, fit the model that you built and compiled in the previous cells. Remember to set your `epochs=1`! 
"""

# Fit your model here
# REMEMBER to set epochs=1

# YOUR CODE HERE
n_epochs = 1
batch_size = 32

history = model.fit(X_train, y_train,
                      batch_size=batch_size, 
                      epochs=n_epochs, 
                      validation_data=(X_test,y_test))

# Visible Test 
n_epochs = len(model.history.history["loss"])
assert n_epochs == 1, 

"""<a id="p2"></a>

### Find the Frog

"""

# Prep to import images (don't alter the code in this cell)
import urllib.request

# Text file of image URLs
text_file = "https://raw.githubusercontent.com/bloominstituteoftechnology/data-science-canvas-images/main/unit_4/sprint_challenge_files/frog_image_url.txt"
# data = urllib.request.urlopen(text_file)

# Create list of image URLs
url_list = [] 
for line in data:
    url_list.append(line.decode('utf-8'))

# Import images (don't alter the code in this cell)

from skimage.io import imread
from skimage.transform import resize 

# instantiate list to hold images
image_list = []


#loop through URLs and load each image
for url in url_list:
    image_list.append(imread(url))

#What is an "image"?
print(type(image_list[0]), end="\n\n")

print("Each of the Images is a Different Size")
print(image_list[0].shape)
print(image_list[1].shape)

"""### Run ResNet50v2
"""

# Imports
import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras.applications.resnet_v2 import ResNet50V2 # <-- pre-trained model 
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet_v2 import preprocess_input, decode_predictions


# YOUR CODE HERE
image_list = [imread(url) for url in url_list]
image_list = [resize(image, (224, 224)) for image in image_list]

img = np.expand_dims(image_list[0], axis=0)

resnet_model = ResNet50V2(weights='imagenet')

features = resnet_model.predict(img)

results = decode_predictions(features, top=5)[0]

results


"""## FIN

"""
